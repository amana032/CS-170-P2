# CS 170 Project 2

# Read in data set and parse as tuples (class, feature 1, feature 2, etc.)
#f = open('CS170_Large_Data__94.txt', "r")
#f = open('CS170_Small_Data__41.txt', "r")

def split_data():
    data = input("Enter the name of the text file (including .txt extension): ")
    try:
        with open(data, 'r') as dataset:
            data = []
            for line in dataset:
                object_to_tuple = tuple(map(float, line.split()))
                data.append(object_to_tuple)
            dataset.close()
        return data

    except FileNotFoundError:
        print("Error: File not found. Please try again.")

def leave_one_out_cross_validation(data, current_set, feature_to_add):
    number_correct = 0
    selected_features = current_set + [feature_to_add]  # Add the new feature

    for i, obj in enumerate(data): # The object we are classifying in the data set
        object_to_classify = [obj[j] for j in selected_features]  # Use only selected features
        object_label = obj[0]

        nearest_neighbor_distance = float('inf')
        nearest_neighbor_label = None

        for k, obj2 in enumerate(data): # Comparing every object to the object we are classifying to find the nearest neighbor
            if i != k: # Don't compare the object to itself
                neighbor_features = [obj2[j] for j in selected_features] # Compare to selected features in nearest neighbor algorithm

                distance = sum((a - b) ** 2 for a, b in zip(object_to_classify, neighbor_features)) ** 0.5
                
                # Nearest neighbor comparison. What is the most similar data point?
                if distance < nearest_neighbor_distance:
                    nearest_neighbor_distance = distance
                    nearest_neighbor_label = obj2[0]

        # All objects have been compared to the object we are classifying, and a label has been determined by shortest distance. Time to check accuracy
        # print(f"Looping over i={i}, object label: {object_label}, predicted label: {nearest_neighbor_label}")

        # Checking for accuracy of classified object
        if object_label == nearest_neighbor_label:
            number_correct += 1

    accuracy = number_correct / len(data)
    return accuracy


def forward_selection_feature_search(data):
    best_set_per_level = [] # Keeps track of the best feature set per level, list of tuples (level, feature set, total accuracy)
    best_feature_set_total = (None, None, 0) # The best feature set overall, to be returned
    current_set = [] # The current set of features, starts empty
    
    print("Beginning search.\n")

    for i in range(1, len(data[0])):
        best_accuracy = 0
        feature_to_add_this_level = -1
        
        print(f"On Level {i} of the search tree")
        
        for k in range(1, len(data[0])): # For each feature, check if it improves accuracy from the previous level set
            if k not in current_set: # Has this feature been added yet? Skip if so
                #print(f" - Considering adding feature {k}:")
                accuracy = leave_one_out_cross_validation(data, current_set, k) # Grab the accuracy of the feature set with this feature added
                #print(f"   - Accuracy: {round(accuracy*100, 2)}%")

                tuple_temp = tuple(set(current_set + [k]))
                print(f"     - Using feature(s) {tuple_temp} accuracy is {round(accuracy*100, 2)}%")

                if accuracy > best_accuracy: # Is this feature better than the best feature so far?
                    best_accuracy = accuracy
                    feature_to_add_this_level = k
        
        # Parsed through all the features! Did we find a one that improved accuracy?
        if feature_to_add_this_level == -1: # No features improved accuracy
            print(f"No features improve accuracy; exiting level {i}")

        else:
            current_set.append(feature_to_add_this_level) # Add the best feature to the current set
            t = (i, current_set.copy(), best_accuracy)
            best_set_per_level.append(t) # Keep a copy of this level in case it's the best
            #print(f"Added most significant feature {feature_to_add_this_level} on level {i} with accuracy {round(best_accuracy*100, 2)}%\n")
            print(f"Feature set {t[1]} was best, accuracy is {round(t[2]*100, 2)}%\n")

    # Print the best feature set at each level
    for i in best_set_per_level:
        print('Level', i[0], 'Feature Set:', i[1], ', Accuracy:', round(i[2]*100, 2), '%')
        if i[2] > best_feature_set_total[2]: # Is this the best feature set so far? Update if so
            best_feature_set_total = i

        elif round(i[2], 2) == round(best_feature_set_total[2], 2): # If the accuracy is the same, pick the one with the fewest features
            if len(i[1]) < len(best_feature_set_total[1]):
                best_feature_set_total = i
    
    print(f"Best feature set: {best_feature_set_total[1]} with accuracy {round(best_feature_set_total[2]*100, 2)}%")
    return best_feature_set_total # Best feature set and accuracy found!


def backwards_elimination_feature_search(data):
    best_set_per_level = [] # Keeps track of the best feature set per level, list of tuples (level, feature set, total accuracy)
    best_feature_set_total = (None, None, 0) # The best feature set overall, to be returned
    current_set = list(range(1, len(data[0]))) # Start with all features

    t = (1, current_set.copy(), leave_one_out_cross_validation(data, current_set, -1)) # Accuracy of all features
    best_set_per_level.append(t) # Keep a copy of this level in case it's the best
    print(f"\nBeginning search, starting with all features: {current_set} with accuracy {round(t[2]*100, 2)}%\n")

    for i in range(2, len(data[0])):
        best_accuracy = 0
        feature_to_remove_this_level = -1

        print(f"On Level {i} of the search tree")
        
        for k in range(1, len(data[0])): # For each feature, check if it improves accuracy from the previous level set
            if k in current_set: # Only remove features in the current set
                #print(f" - Considering removing feature {k}:")
                accuracy = leave_one_out_cross_validation(data, current_set, k) # Grab the accuracy of the feature set with this feature removed
                #print(f"   - Accuracy: {round(accuracy*100, 2)}%")

                tuple_temp = tuple(set(current_set + [k]))
                print(f"     - Using feature(s) {tuple_temp} accuracy is {round(accuracy*100, 2)}%")

                if accuracy > best_accuracy: # Is this feature worse than the worst so far?
                    best_accuracy = accuracy
                    feature_to_remove_this_level = k
        
        # Parsed through all the features! Did we find a one that improved accuracy?
        if feature_to_remove_this_level == -1: # No features improved accuracy
            print(f"No features improve accuracy; exiting level {i}")

        else:
            current_set.remove(feature_to_remove_this_level) # Remove the best feature from the current set
            t = (i, current_set.copy(), best_accuracy)
            best_set_per_level.append(t) # Keep a copy of this level in case it's the best
            #print(f"Removed least significant feature {feature_to_remove_this_level} on level {i} to create set {current_set} with accuracy {round(best_accuracy*100, 2)}%\n")
            print(f"Feature set {t[1]} was best, accuracy is {round(t[2]*100, 2)}%\n")

    # Print the best feature set at each level
    for i in best_set_per_level:
        print('Level', i[0], 'Feature set:', i[1], ', Accuracy:', round(i[2]*100, 2), '%')
        if i[2] > best_feature_set_total[2]: # Is this the best feature set so far? Update if so
            best_feature_set_total = i

        elif round(i[2], 2) == round(best_feature_set_total[2], 2): # If the accuracy is the same, pick the one with the fewest features
            if len(i[1]) < len(best_feature_set_total[1]):
                best_feature_set_total = i
    
    print(f"\nBest feature set: {best_feature_set_total[1]} with accuracy {round(best_feature_set_total[2]*100, 2)}%")
    return best_feature_set_total # Best feature set and accuracy found!

def main():
    print("Welcome to the Alexis Manalastas' Feature Selection Algorithm.")
    data = split_data()

    if data is None:
        print("Exiting...")
        return
    
    print("Type the number of the algorithm you want to run.")
    print("1) Forward Selection")
    print("2) Backward Elimination")
    print("3) Exit")

    case = input("Selection: ")
    while (True): # Sorry...
        if int(case) == 1:
            forward_selection_feature_search(data)
            break
        elif int(case) == 2:
            backwards_elimination_feature_search(data)
            break
        elif int(case) == 3:
            break
        else:
            print("Invalid input. Please try again.\n")

            print("Type the number of the algorithm you want to run.")
            print("1) Forward Selection")
            print("2) Backward Elimination")
            print("3) Exit")
            case = input("Selection: ")

    print("\nExiting...")

# __name__
if __name__=="__main__":
    main()